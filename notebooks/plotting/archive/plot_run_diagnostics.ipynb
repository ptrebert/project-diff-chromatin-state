{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         bins  binsize  last_start    size_bp\n",
      "name                                         \n",
      "chr1  1244782      200   248956200  248956422\n",
      "chr2  1210967      200   242193200  242193529\n",
      "chr3   991477      200   198295200  198295559\n",
      "chr4   951072      200   190214200  190214555\n",
      "chr5   907691      200   181538000  181538259\n",
      "      ka_lambda      ka_h      ka_k  expected_score   var_len  score_m5_prob  \\\n",
      "chr1   0.773629  1.457639  0.407399       -2.804324  352527.0       0.544361   \n",
      "chr2   0.795679  1.445842  0.406808       -2.680350  370046.0       0.485004   \n",
      "chr3   0.830211  1.579600  0.415902       -2.834637  312980.0       0.518472   \n",
      "chr4   0.971805  2.116375  0.427568       -3.213663  299359.0       0.559525   \n",
      "chr5   0.865906  1.705126  0.421199       -2.906863  274256.0       0.510586   \n",
      "\n",
      "      score_m4_prob  score_m3_prob  score_m2_prob  score_m1_prob  \\\n",
      "chr1            0.0       0.086805       0.004968       0.128694   \n",
      "chr2            0.0       0.137325       0.003455       0.140210   \n",
      "chr3            0.0       0.129111       0.003614       0.132465   \n",
      "chr4            0.0       0.170924       0.001491       0.107403   \n",
      "chr5            0.0       0.163496       0.002502       0.122544   \n",
      "\n",
      "      score_0_prob  score_p1_prob  score_p2_prob  score_p3_prob  \\\n",
      "chr1      0.064266       0.045614       0.113760       0.002739   \n",
      "chr2      0.069003       0.042433       0.113643       0.001663   \n",
      "chr3      0.060845       0.041854       0.105017       0.001626   \n",
      "chr4      0.048968       0.028015       0.077384       0.000811   \n",
      "chr5      0.057722       0.036547       0.098816       0.001218   \n",
      "\n",
      "      score_p4_prob     size  \n",
      "chr1       0.008793  1244782  \n",
      "chr2       0.007265  1210967  \n",
      "chr3       0.006996   991477  \n",
      "chr4       0.005477   951072  \n",
      "chr5       0.006569   907691  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9f02ee1e86df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mscore_md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd_chrom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_param_dep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleg_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msave_figures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os as os\n",
    "import collections as col\n",
    "import itertools as itt\n",
    "import pickle as pck\n",
    "import time as ti\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# What does this do?\n",
    "# Plot several diagnostic plots\n",
    "# that illustrate the adherence to\n",
    "# the assumptions behind the\n",
    "# Karlin-Altschul statistics\n",
    "\n",
    "date = '20180604'\n",
    "\n",
    "run_diagnostics = False\n",
    "run_blockmax = True\n",
    "run_excess = False\n",
    "run_korder = False\n",
    "\n",
    "show_figures = True\n",
    "save_figures = False\n",
    "\n",
    "sns.set(style='white',\n",
    "        font_scale=1.5,\n",
    "        rc={'font.family': ['sans-serif'],\n",
    "            'font.sans-serif': ['DejaVu Sans']})\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "scorings = ['emission', 'replicate']\n",
    "resolution = {'lo': 150, 'hi': 350}\n",
    "\n",
    "fhgfs_base = '/TL/deep/fhgfs/projects/pebert/thesis/projects/statediff'\n",
    "cache_dir = os.path.join(fhgfs_base, 'caching/notebooks')\n",
    "\n",
    "data_root = os.path.join(fhgfs_base, 'sciddo', 'deep')\n",
    "\n",
    "base_out = '/TL/deep-external01/nobackup/pebert/cloudshare/mpiinf/phd/chapter_projects/statediff'\n",
    "fig_supp = os.path.join(base_out, 'figures', 'pub', 'supp')\n",
    "fig_main = os.path.join(base_out, 'figures', 'pub', 'main')\n",
    "fig_collect = os.path.join(base_out, 'figures', 'pub', 'collection')\n",
    "\n",
    "def collect_block_maxima(fpaths, scoring, cache_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(cache_file):\n",
    "        block_max = col.defaultdict(list)\n",
    "        for fp in fpaths:\n",
    "            with pd.HDFStore(fp, 'r') as hdf:\n",
    "                load_keys = [k for k in hdf.keys() if k.startswith('/sstrun/{}'.format(scoring))]\n",
    "                for k in load_keys:\n",
    "                    if k.endswith('/thresholds'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    data = data.loc[data['is_hsp'] == 1, :].copy()\n",
    "                    chrom = os.path.split(k)[1]\n",
    "                    data['comparison'] = data['sample1'] + '-VS-' + data['sample2']\n",
    "                    for comp in data['comparison'].unique():\n",
    "                        sub = data.loc[data['comparison'] == comp, 'norm_nat_score']\n",
    "                        block_max[chrom].append(float(sub.max()))\n",
    "        ts = ti.ctime()\n",
    "        with open(cache_file, 'w') as cache:\n",
    "            js.dump({'timestamp': ts, 'data': block_max},\n",
    "                     cache, sort_keys=True, indent=1)\n",
    "    else:\n",
    "        with open(cache_file, 'r') as cache:\n",
    "            cached = js.load(cache)\n",
    "            block_max = cached['data']\n",
    "    return block_max\n",
    "\n",
    "def collect_blockmax_comp(fpaths, scorings, cache_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(cache_file):\n",
    "        block_max = dict()\n",
    "        for s in scorings:\n",
    "            block_max[s] = col.defaultdict(list)\n",
    "        for fp in fpaths:\n",
    "            with pd.HDFStore(fp, 'r') as hdf:\n",
    "                for k in hdf.keys():\n",
    "                    if k.endswith('/thresholds'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    data = data.loc[data['is_hsp'] == 1, :].copy()\n",
    "                    _, _, scoring, chrom = k.split('/')\n",
    "                    fname = os.path.basename(fp).split('.')[0]\n",
    "                    parts = fname.split('_')\n",
    "                    s1 = parts[2]\n",
    "                    s2 = parts[5]\n",
    "                    plot_comp = s1 + '_vs_' + s2\n",
    "                    data['comparison'] = data['sample1'] + '_VS_' + data['sample2']\n",
    "                    for comp in data['comparison'].unique():\n",
    "                        sub = data.loc[data['comparison'] == comp, 'norm_nat_score']\n",
    "                        block_max[scoring][plot_comp].append(float(sub.max()))\n",
    "        ts = ti.ctime()\n",
    "        with open(cache_file, 'w') as cache:\n",
    "            js.dump({'timestamp': ts, 'data': block_max},\n",
    "                     cache, sort_keys=True, indent=1) \n",
    "    else:\n",
    "        with open(cache_file, 'r') as cache:\n",
    "            cached = js.load(cache)\n",
    "            block_max = cached['data']\n",
    "    return block_max\n",
    "\n",
    "def read_thresholds(fpath, score):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    thresholds = dict()\n",
    "    with pd.HDFStore(fpath, 'r') as hdf:\n",
    "        t_data = hdf['/sstrun/{}/thresholds'.format(score)]\n",
    "        for c in t_data.columns:\n",
    "            if c.endswith('_lo'):\n",
    "                thresholds[c.split('_')[0]] = {'value': col.defaultdict(float),\n",
    "                                               'percent': col.defaultdict(float)}\n",
    "        for row in t_data.itertuples():\n",
    "            thresholds['ferreira']['value'][row.index] = row.ferreira_lo\n",
    "            thresholds['ferreira']['percent'][row.index] = row.ferreira_pct\n",
    "            thresholds['loretan']['value'][row.index] = row.loretan_lo\n",
    "            thresholds['loretan']['percent'][row.index] = row.loretan_pct\n",
    "            thresholds['quantile']['value'][row.index] = row.quantile_lo\n",
    "            thresholds['quantile']['percent'][row.index] = row.quantile_pct\n",
    "    return thresholds\n",
    "\n",
    "def collect_excess_scores(fpaths, score, cache_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(cache_file):\n",
    "        excess_scores = {'ferreira': col.defaultdict(list),\n",
    "                         'loretan': col.defaultdict(list),\n",
    "                         'quantile': col.defaultdict(list)}\n",
    "        for fp in fpaths:\n",
    "            thresholds = read_thresholds(fp, score)\n",
    "            with pd.HDFStore(fp, 'r') as hdf:\n",
    "                load_keys = [k for k in hdf.keys() if k.startswith('/sstrun/{}'.format(score))]\n",
    "                for k in load_keys:\n",
    "                    if k.endswith('/thresholds'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    data = data.loc[data['is_hsp'] == 1, :].copy()\n",
    "                    chrom = os.path.split(k)[1]\n",
    "                    data['comparison'] = data['sample1'] + '-VS-' + data['sample2']\n",
    "                    for comp in data['comparison'].unique():\n",
    "                        sub = data.loc[data['comparison'] == comp, 'norm_nat_score']\n",
    "                        for rule in ['ferreira', 'loretan', 'quantile']:\n",
    "                            lower_bound = thresholds[rule]['value'][chrom]\n",
    "                            excess = sub.loc[sub > lower_bound].copy()\n",
    "                            excess -= lower_bound\n",
    "                            excess_scores[rule][chrom].extend(list(map(float, excess)))\n",
    "        ts = ti.ctime()\n",
    "        with open(cache_file, 'w') as cache:\n",
    "            js.dump({'timestamp': ts, 'data': excess_scores},\n",
    "                     cache, sort_keys=True, indent=1)\n",
    "    else:\n",
    "        with open(cache_file, 'r') as cache:\n",
    "            cached = js.load(cache)\n",
    "            excess_scores = cached['data']\n",
    "    return excess_scores\n",
    "\n",
    "def collect_order_scores(fpaths, score, cache_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(cache_file):\n",
    "        ranked_scores = []\n",
    "        for fp in fpaths:\n",
    "            with pd.HDFStore(fp, 'r') as hdf:\n",
    "                load_keys = [k for k in hdf.keys() if k.startswith('/sstrun/{}'.format(score))]\n",
    "                for k in load_keys:\n",
    "                    if k.endswith('/thresholds'):\n",
    "                        continue\n",
    "                    data = hdf[k]\n",
    "                    data = data.loc[data['is_hsp'] == 1, :].copy()\n",
    "                    data['comparison'] = data['sample1'] + '-VS-' + data['sample2']\n",
    "                    for comp in data['comparison'].unique():\n",
    "                        sub = data.loc[data['comparison'] == comp, ['norm_nat_score']]\n",
    "                        sub['rank'] = sub['norm_nat_score'].rank(method='dense', ascending=False)\n",
    "                        ranked_scores.append(sub)\n",
    "        ranked_scores = pd.concat(ranked_scores, axis=0, ignore_index=False)\n",
    "        with pd.HDFStore(cache_file, 'w') as hdf:\n",
    "            hdf.put('data', ranked_scores, format='fixed')\n",
    "            hdf.flush()\n",
    "    else:\n",
    "        with pd.HDFStore(cache_file, 'r') as hdf:\n",
    "            ranked_scores = hdf['data']\n",
    "    return ranked_scores\n",
    "\n",
    "\n",
    "def sort_label_chroms(chroms):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    chrom_sort = []\n",
    "    for c in chroms:\n",
    "        try:\n",
    "            chrom_num = int(c.strip('chr'))\n",
    "            chrom_sort.append((chrom_num, c.strip('chr'), c))\n",
    "        except ValueError:\n",
    "            chrom_num = 23\n",
    "            chrom_sort.append((chrom_num, 'X', c))\n",
    "    return sorted(chrom_sort)\n",
    "    \n",
    "\n",
    "def plot_block_maxima(bmax, scoring):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    chrom_sort = sort_label_chroms(list(bmax.keys()))\n",
    "    plot_data = [bmax[t[2]] for t in chrom_sort]\n",
    "    x_labels = [t[1] for t in chrom_sort]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    medianprops = {'color': 'darkorange', 'linewidth': 2}\n",
    "    boxprops = {'color': 'dodgerblue', 'linewidth': 2}\n",
    "    whiskerprops = {'color': 'dodgerblue'}\n",
    "    capprops = {'color': 'dodgerblue'}\n",
    "    flierprops = {'color': 'skyblue', 'marker': 'o',\n",
    "                  'markerfacecolor': 'white', 'markersize': 8,\n",
    "                  'markeredgecolor': 'cornflowerblue'}\n",
    "    \n",
    "    ax.set_ylim(0, 15000)\n",
    "    \n",
    "    ax.boxplot(plot_data, notch=False, labels=x_labels,\n",
    "               boxprops=boxprops, medianprops=medianprops,\n",
    "               whiskerprops=whiskerprops, capprops=capprops,\n",
    "               flierprops=flierprops)\n",
    "    \n",
    "    ax.set_title('Maximal scores per chromosome - {} scoring'.format(scoring),\n",
    "                 fontsize=14)\n",
    "    \n",
    "    ax.set_xlabel('Chromosomes', fontsize=12)\n",
    "    ax.set_ylabel('Maximal scores', fontsize=12)\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    return fig, []\n",
    "    \n",
    "\n",
    "def plot_qq_comp(plot_data, figtitle):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    total_subplots = len(plot_data.keys())\n",
    "    total_subplots = int(scisp.comb(total_subplots, 2, repetition=False))\n",
    "    num_cols = 3\n",
    "    num_rows = total_subplots // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols,\n",
    "                             sharex=False, sharey=False,\n",
    "                             figsize=(16, 20))\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    ex_title = fig.text(0.5, 0.89, figtitle, ha='center', fontsize=16)\n",
    "    ex_xlabel = fig.text(0.5, 0.1, 'Score quantiles of comparison 1',\n",
    "                         ha='center', fontsize=16)\n",
    "    ex_ylabel = fig.text(0.1, 0.5, 'Score quantiles of comparison 2',\n",
    "                         va='center', rotation='vertical', fontsize=16)\n",
    "    \n",
    "    plot_order = sorted(plot_data.keys())\n",
    "    r, c = 0, 0\n",
    "    done = set()\n",
    "    for x_comp in plot_order:\n",
    "        for y_comp in plot_order:\n",
    "            if x_comp == y_comp:\n",
    "                continue\n",
    "            if (x_comp, y_comp) in done or (y_comp, x_comp) in done:\n",
    "                continue\n",
    "            label_vals = x_comp.split('_') + y_comp.split('_')\n",
    "            label_vals = tuple([label_vals[i] for i in [0,2,3,5]])\n",
    "            label = 'x: {} - {}\\ny: {} - {}'.format(*label_vals)\n",
    "            ax = axes[r, c]\n",
    "            c += 1\n",
    "            if c == 3:\n",
    "                c = 0\n",
    "                r += 1\n",
    "            x_vals = sorted(plot_data[x_comp])\n",
    "            y_vals = sorted(plot_data[y_comp])\n",
    "            if len(x_vals) < len(y_vals):\n",
    "                est_quant = np.linspace(0, 1, num=len(x_vals), endpoint=True, dtype=np.float16)\n",
    "                y_vals = mstats.mquantiles(y_vals, prob=est_quant, alphap=0, betap=1)\n",
    "            elif len(x_vals) > len(y_vals):\n",
    "                est_quant = np.linspace(0, 1, num=len(y_vals), endpoint=True, dtype=np.float16)\n",
    "                x_vals = mstats.mquantiles(x_vals, prob=est_quant, alphap=0, betap=1)                \n",
    "            else:\n",
    "                pass\n",
    "            ax.set(adjustable='box')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.plot(x_vals, y_vals, c='dodgerblue', lw=2)\n",
    "            # manually add best fit reg line\n",
    "            slope, inter, rvalue, pv, graderr = stats.linregress(x_vals, y_vals)\n",
    "            reg_yvals = np.array(x_vals) * slope + inter\n",
    "            ax.plot(x_vals, reg_yvals, c='darkorange', lw=3, label='$R$ = ' + str(np.round(rvalue, 3)) )\n",
    "            ax.legend(loc='lower right', fontsize=14)\n",
    "            # add label to plot\n",
    "            ax.text(0.05, 0.8, label, fontsize=14, transform=ax.transAxes)\n",
    "            done.add((x_comp, y_comp))\n",
    "            done.add((y_comp, x_comp))\n",
    "    return fig, []\n",
    "   \n",
    "def plot_probplot(data, figtitle, theo_dist, is_chrom=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dist = {'gumbel_r': stats.gumbel_r.fit,\n",
    "            'expon': stats.expon.fit,\n",
    "            'weibull_max': stats.weibull_max.fit,\n",
    "            'genextreme': stats.genextreme.fit,\n",
    "            'genpareto': stats.genpareto.fit}\n",
    "    total_subplots = len(data.keys()) + 1\n",
    "    num_cols = 4\n",
    "    num_rows = total_subplots // num_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols,\n",
    "                             sharex=False, sharey=False,\n",
    "                             figsize=(14, 20))\n",
    "    fig_title = figtitle\n",
    "    fig_xlabel = 'Theoretical quantiles'\n",
    "    fig_ylabel = 'Observed values'\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    ex_title = fig.text(0.5, 0.89, fig_title, ha='center', fontsize=16)\n",
    "    ex_xlabel = fig.text(0.5, 0.1, fig_xlabel, ha='center', fontsize=16)\n",
    "    ex_ylabel = fig.text(0.1, 0.5, fig_ylabel, va='center', rotation='vertical', fontsize=16)\n",
    "    \n",
    "    if is_chrom:\n",
    "        plot_infos = sort_label_chroms(list(data.keys()))\n",
    "        plot_infos.append((24, 'gen.', 'genome'))\n",
    "    else:\n",
    "        plot_infos = sorted([(k, k, k) for k in data.keys()])\n",
    "    \n",
    "    r, c = 0, 0\n",
    "    gw_data = []\n",
    "    for num, short, name in plot_infos:\n",
    "        ax = axes[r, c]\n",
    "        c += 1\n",
    "        if c == 4:\n",
    "            c = 0\n",
    "            r += 1\n",
    "        try:\n",
    "            chrom_data = data[name]\n",
    "            gw_data += chrom_data\n",
    "        except KeyError:\n",
    "            chrom_data = gw_data\n",
    "        try:\n",
    "            est_param = dist[theo_dist](chrom_data)\n",
    "        except FloatingPointError:\n",
    "            print('Skipping {}'.format(name))\n",
    "            continue\n",
    "        (osm, osr), (slope, inter, cod) = stats.probplot(chrom_data, est_param,\n",
    "                                                         dist=theo_dist, fit=True, plot=ax)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # markers\n",
    "        ax.get_lines()[0].set_color('dodgerblue')\n",
    "        # trendline\n",
    "        ax.get_lines()[1].set_color('darkorange')\n",
    "        ax.set_title('')\n",
    "        \n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        \n",
    "        if is_chrom:\n",
    "            ax.text(0.05, 0.9, name, fontsize=14, transform=ax.transAxes)\n",
    "        else:\n",
    "            ax.text(0.05, 0.9, 'k = {}'.format(name), fontsize=14, transform=ax.transAxes)\n",
    "        \n",
    "        cod = str(np.round(cod, 3))\n",
    "        ax.text(0.6, 0.1, '$R$ = ' + cod, fontsize=12, transform=ax.transAxes)\n",
    "           \n",
    "    return fig, [ex_title, ex_xlabel, ex_ylabel]\n",
    "\n",
    "def plot_joint_fit(hsps, rands):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    x_values = np.linspace(rands.min(), hsps.max(), 10000)\n",
    "    \n",
    "    hsps_loc, hsps_scale = stats.gumbel_r.fit(hsps)\n",
    "    hsps_exp = stats.gumbel_r.pdf(x_values,\n",
    "                                  loc=hsps_loc,\n",
    "                                  scale=hsps_scale)\n",
    "    \n",
    "    rands_loc, rands_scale = stats.gumbel_l.fit(rands)\n",
    "    rands_exp = stats.gumbel_l.pdf(x_values,\n",
    "                                   loc=rands_loc,\n",
    "                                   scale=rands_scale)\n",
    "    \n",
    "    _ = ax.hist(np.concatenate([hsps, rands]), normed=True,\n",
    "                bins=100, alpha=0.4, color='blue',\n",
    "                histtype='stepfilled', label='Observed / sampled')\n",
    "    \n",
    "    _ = ax.plot(x_values, rands_exp, lw=2, color='black', label='Fit (Gumbel [L])')\n",
    "\n",
    "    _ = ax.plot(x_values, hsps_exp, lw=2, color='darkorange', label='Fit (Gumbel [R])')\n",
    "    \n",
    "    _ = ax.legend(loc='upper center', fontsize=12)\n",
    "    _ = ax.set_title('Observed and random scores', fontsize=16)\n",
    "    \n",
    "    return fig, []\n",
    "\n",
    "    \n",
    "def plot_dist_fit(obs_data, title, gumbel_type):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_values = np.linspace(obs_data.min(), obs_data.max(), 100)\n",
    "    \n",
    "    if gumbel_type == 'right':\n",
    "        gumbel_label = 'Fit (Gumbel [R])'\n",
    "        gumbel_loc, gumbel_scale = stats.gumbel_r.fit(obs_data)\n",
    "        gumbel_exp = stats.gumbel_r.pdf(x_values,\n",
    "                                        loc=gumbel_loc,\n",
    "                                        scale=gumbel_scale)\n",
    "    else:\n",
    "        gumbel_label = 'Fit (Gumbel [L])'\n",
    "        gumbel_loc, gumbel_scale = stats.gumbel_l.fit(obs_data)\n",
    "        gumbel_exp = stats.gumbel_l.pdf(x_values,\n",
    "                                        loc=gumbel_loc,\n",
    "                                        scale=gumbel_scale)\n",
    "    \n",
    "    norm_loc, norm_scale = stats.norm.fit(obs_data)\n",
    "    norm_exp = stats.norm.pdf(x_values,\n",
    "                              loc=norm_loc,\n",
    "                              scale=norm_scale)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    _ = ax.hist(obs_data, normed=True, bins=100, alpha=0.5,\n",
    "                histtype='stepfilled', color='blue', label='Observed')\n",
    "\n",
    "    _ = ax.plot(x_values, norm_exp, lw=4, color='black', label='Fit (Norm)')\n",
    "\n",
    "    _ = ax.plot(x_values, gumbel_exp, lw=5, color='darkorange',\n",
    "                label=gumbel_label, alpha=0.8)\n",
    "\n",
    "    _ = ax.legend(loc='upper center', fontsize=12)\n",
    "    _ = ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    return fig, []\n",
    "\n",
    "\n",
    "def plot_bin_scores(hsps, rands):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    joint_values = np.concatenate([hsps, rands])\n",
    "    \n",
    "    x_values = np.linspace(joint_values.min(), joint_values.max(), 100)\n",
    "    \n",
    "    hsp_loc, hsp_scale = stats.norm.fit(hsps)\n",
    "    hsp_exp = stats.norm.pdf(x_values,\n",
    "                             loc=hsp_loc,\n",
    "                             scale=hsp_scale)\n",
    "    \n",
    "    rand_loc, rand_scale = stats.norm.fit(rands)\n",
    "    rand_exp = stats.norm.pdf(x_values,\n",
    "                              loc=rand_loc,\n",
    "                              scale=rand_scale)\n",
    "    \n",
    "    \n",
    "    ax.hist(joint_values, normed=True, bins=100, alpha=0.5,\n",
    "            histtype='stepfilled', color='blue', label='Observed / random')\n",
    "    \n",
    "    ax.plot(x_values, rand_exp, color='black',\n",
    "            alpha=0.8, lw=4)\n",
    "    \n",
    "    ax.plot(x_values, hsp_exp, color='darkorange',\n",
    "            alpha=0.8, lw=4)\n",
    "\n",
    "    return fig, []\n",
    "\n",
    "\n",
    "if run_diagnostics:\n",
    "    if run_blockmax:\n",
    "        # step 1: modeling based on block maxima\n",
    "        for score in scorings:\n",
    "            score_cache = os.path.join(cache_dir, 'blockmax_{}.json'.format(score))\n",
    "            bm = collect_block_maxima(run_files, score, score_cache)\n",
    "            fig, exart = plot_block_maxima(bm, score)\n",
    "            if save_figures:\n",
    "                outpath = os.path.join(fig_supp, 'fig_X_supp_blockmax_{}.svg'.format(score))\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight')\n",
    "                outpath = outpath.replace('.svg', '.png')\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight', dpi=300)\n",
    "            title = 'Probability plots for (GEV) Gumbel distribution - {} scoring'.format(score)  \n",
    "            fig, exart = plot_probplot(bm, title, 'gumbel_r')\n",
    "            if save_figures:\n",
    "                outpath = os.path.join(fig_supp, 'fig_X_supp_bmax-gumbel_{}.svg'.format(score))\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight')\n",
    "                outpath = outpath.replace('.svg', '.png')\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight', dpi=300)\n",
    "        # check that individual comparisons\n",
    "        # give rise to roughly the same distribution\n",
    "        score_cache = os.path.join(cache_dir, 'blockmax_comp.json')\n",
    "        bm = collect_blockmax_comp(run_files, scorings, score_cache)\n",
    "        for score in scorings:\n",
    "            title = 'QQ-plot for maximal segment scores - {} scoring'.format(score)\n",
    "            fig, exart = plot_qq_comp(bm[score], title)\n",
    "            if save_figures:\n",
    "                outpath = os.path.join(fig_supp, 'fig_X_supp_qqplot_{}.svg'.format(score))\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight')\n",
    "                outpath = outpath.replace('.svg', '.png')\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "\n",
    "    if run_excess:\n",
    "        # step 2: modeling based on threshold excess\n",
    "        # empirical rules of thumb seem not to be\n",
    "        # appropriate for this data\n",
    "        # No GPD/EXP fit for threshold excess\n",
    "        for score in scorings:\n",
    "            score_cache = os.path.join(cache_dir, 'excess_{}.json'.format(score))\n",
    "            exc = collect_excess_scores(run_files, score, score_cache)\n",
    "            for trule in ['ferreira', 'loretan', 'quantile']:\n",
    "                exscores = exc[trule]\n",
    "                title = 'Prob. plots for (GPD) EXP dist.: {} scoring / {} threshold'.format(score, trule.capitalize())\n",
    "                fig, exart = plot_probplot(exscores, title, 'expon')\n",
    "\n",
    "    if run_korder:\n",
    "        # step 3: modeling based on k-order statistics\n",
    "        for score in scorings:\n",
    "            score_cache = os.path.join(cache_dir, 'orderstat_{}.h5'.format(score))\n",
    "            ordstat = collect_order_scores(run_files, score, score_cache)\n",
    "            plot_data = dict()\n",
    "            for k in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 90, 100]:\n",
    "                k_data = ordstat.loc[ordstat['rank'] < k + 1, 'norm_nat_score'].copy()\n",
    "                plot_data[k] = k_data.values.tolist()\n",
    "            title = 'Prob. plots for (GEV) Gumbel dist. - {} scoring, k-th order statistic'.format(score)  \n",
    "            fig, exart = plot_probplot(plot_data, title, 'gumbel_r', False)\n",
    "            if save_figures:\n",
    "                outpath = os.path.join(fig_supp, 'fig_X_supp_kord-gumbel_{}.svg'.format(score))\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight')\n",
    "                outpath = outpath.replace('.svg', '.png')\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight', dpi=300)\n",
    "\n",
    "            title = 'Prob. plots for (GPD) EXP dist. - {} scoring, k-th order statistic'.format(score)  \n",
    "            fig, exart = plot_probplot(plot_data, title, 'expon', False)\n",
    "            if save_figures:\n",
    "                outpath = os.path.join(fig_supp, 'fig_X_supp_kord-expon_{}.svg'.format(score))\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight')\n",
    "                outpath = outpath.replace('.svg', '.png')\n",
    "                fig.savefig(outpath, bbox_extra_artists=exart, bbox_inches='tight', dpi=300)\n",
    "            \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
